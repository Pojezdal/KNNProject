{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pojezdal/KNNProject/blob/Denis/source/KNN_compressai_xpojez00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp1qXVZm37WO"
      },
      "source": [
        "#KNN Projekt\n",
        "\n",
        "Denis Pojezdál xpojez00\\\n",
        "Ivan Rachler xrachl00\\\n",
        "Sebastián Chupáč xchupa03\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry-4OF1CFXFX",
        "outputId": "28ea415d-5223-40e1-a4d2-7c1e6b372f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.0->lpips)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lpips\n",
            "Successfully installed lpips-0.1.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting piq\n",
            "  Downloading piq-0.8.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from piq) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->piq) (1.25.2)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->piq) (2.2.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->piq) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision>=0.10.0->piq) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision>=0.10.0->piq) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision>=0.10.0->piq) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision>=0.10.0->piq) (1.3.0)\n",
            "Installing collected packages: piq\n",
            "Successfully installed piq-0.8.0\n",
            "Collecting compressai\n",
            "  Downloading compressai-1.2.6.tar.gz (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops (from compressai)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from compressai) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from compressai) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from compressai) (1.11.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from compressai) (3.7.1)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from compressai) (2.2.1+cu121)\n",
            "Collecting torch-geometric>=2.3.0 (from compressai)\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from compressai) (4.11.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from compressai) (0.17.1+cu121)\n",
            "Collecting pytorch-msssim (from compressai)\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from compressai) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->compressai) (12.4.127)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->compressai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->compressai) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->compressai) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->compressai) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric>=2.3.0->compressai) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric>=2.3.0->compressai) (3.4.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->compressai) (1.3.0)\n",
            "Building wheels for collected packages: compressai\n",
            "  Building wheel for compressai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressai: filename=compressai-1.2.6-cp310-cp310-linux_x86_64.whl size=400494 sha256=8ec692779e64f68c96b82d86fddd490b92bc194432dca4f3e9197516a70739da\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/e1/85/87edc5d40a531877f35ba1cfc8f66e2e76d49d4845f57c0f46\n",
            "Successfully built compressai\n",
            "Installing collected packages: einops, torch-geometric, pytorch-msssim, compressai\n",
            "Successfully installed compressai-1.2.6 einops-0.8.0 pytorch-msssim-1.0.0 torch-geometric-2.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install lpips\n",
        "!pip install piq\n",
        "!pip install compressai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COC68u5O4S6H"
      },
      "source": [
        "Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvxFNGtS3mG6",
        "outputId": "63cc9818-b218-4dd1-ff10-fc69301ad129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import piq\n",
        "import lpips\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "from enum import Enum\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from google.colab import drive\n",
        "import compressai\n",
        "from compressai.optimizers import net_aux_optimizer\n",
        "from compressai.models import FactorizedPrior\n",
        "from compressai.models.utils import conv, deconv\n",
        "from compressai.layers import GDN, MaskedConv2d\n",
        "from compressai.entropy_models import EntropyBottleneck, GaussianConditional\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8gX3KIP4p5f"
      },
      "source": [
        "Choose device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyJnF2qp4pF1",
        "outputId": "8e68f1e8-b77c-4311-d8e6-6cf2fcc3d056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on the GPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNaz5sN744Ar"
      },
      "source": [
        "Dataset and dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UgZrv8343kp",
        "outputId": "32029de9-b0f1-4a89-df11-aecf7f1684bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to /content/drive/My Drive/knn_datasets/stl10_binary.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2640397119/2640397119 [04:45<00:00, 9241392.78it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/My Drive/knn_datasets/stl10_binary.tar.gz to /content/drive/My Drive/knn_datasets/\n",
            "Train size: 80000\n"
          ]
        }
      ],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, dataset, ratio = [0.75, 0.2, 0.05], batch_size = 64, name = None):\n",
        "        self.dataset = dataset\n",
        "        train_data, val_data, test_data = torch.utils.data.random_split(dataset, [int(len(dataset) * ratio) for ratio in ratio])\n",
        "        self.train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "        self.val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "        self.test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "        self.batch_size = batch_size\n",
        "        self.name = name if name is not None else \"Not specified\"\n",
        "        print(f\"Train size: {len(train_data)}\")\n",
        "\n",
        "stl10_dataset = torchvision.datasets.STL10(root='/content/drive/My Drive/knn_datasets/', split='unlabeled', transform=torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]), download=True)\n",
        "\n",
        "stl10 = DataLoader(\n",
        "    stl10_dataset,\n",
        "    ratio = [0.8, 0.2, 0.0],\n",
        "    name = \"STL\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIPkjhsk5L88"
      },
      "source": [
        "Data augmentation if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SfaSfm835PIg"
      },
      "outputs": [],
      "source": [
        "#Insert Data augmentation, trainstats, scheduler, if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "to_tlwTes_UA"
      },
      "outputs": [],
      "source": [
        "class FactorizedPriorReduced(FactorizedPrior):\n",
        "    def __init__(self, N, M, **kwargs):\n",
        "        super().__init__(N, M, **kwargs)\n",
        "\n",
        "        self.entropy_bottleneck = EntropyBottleneck(M)\n",
        "\n",
        "        self.g_a = nn.Sequential(\n",
        "            conv(3, N),\n",
        "            GDN(N),\n",
        "            conv(N, N),\n",
        "            GDN(N),\n",
        "            conv(N, N),\n",
        "            GDN(N),\n",
        "            conv(N, M),\n",
        "        )\n",
        "\n",
        "        self.g_s = nn.Sequential(\n",
        "            deconv(M, N),\n",
        "            GDN(N, inverse=True),\n",
        "            deconv(N, N),\n",
        "            GDN(N, inverse=True),\n",
        "            deconv(N, N),\n",
        "            GDN(N, inverse=True),\n",
        "            deconv(N, 3),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def downsampling_factor(self) -> int:\n",
        "        return 2**4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vTPLVdzXzDtI"
      },
      "outputs": [],
      "source": [
        "class TrainConfig:\n",
        "    def __init__(self, model, epochs, dataloader, loss_metric, loss_alpha, main_optimizer, aux_optimizer, main_scheduler = None, aux_scheduler = None,\n",
        "                 save_path = None, custom_name = None, start_epoch = 0, best_loss = float('inf'), loss_transform = None, total_time = 0, load_model = None):\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.dataloader = dataloader\n",
        "        self.loss_metric = loss_metric\n",
        "        self.loss_alpha = loss_alpha\n",
        "        self.main_optimizer = main_optimizer\n",
        "        self.aux_optimizer = aux_optimizer\n",
        "        self.main_scheduler = main_scheduler\n",
        "        self.aux_scheduler = aux_scheduler\n",
        "        self.save_path = save_path if save_path else \"/content/drive/My Drive/knn_models\"\n",
        "        if not os.path.exists(self.save_path):\n",
        "            os.makedirs(self.save_path)\n",
        "        if not os.path.exists(self.save_path + \"/logs\"):\n",
        "            os.makedirs(self.save_path + \"/logs\")\n",
        "        self.custom_name = custom_name if custom_name else time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.start_epoch = start_epoch\n",
        "        self.best_loss = best_loss\n",
        "        self.loss_transform = loss_transform\n",
        "        self.total_time = total_time\n",
        "\n",
        "        self.load_model = load_model\n",
        "        if self.load_model:\n",
        "            self.load_from_checkpoint(self.load_model)\n",
        "\n",
        "    def load_from_checkpoint(self, path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        self.model.load_state_dict(checkpoint['model_state'])\n",
        "        self.main_optimizer.load_state_dict(checkpoint['main_optimizer_state'])\n",
        "        self.aux_optimizer.load_state_dict(checkpoint['aux_optimizer_state'])\n",
        "        if self.main_scheduler and checkpoint['main_scheduler_state']:\n",
        "            self.main_scheduler.load_state_dict(checkpoint['main_scheduler_state'])\n",
        "        if self.aux_scheduler and checkpoint['aux_scheduler_state']:\n",
        "            self.aux_scheduler.load_state_dict(checkpoint['aux_scheduler_state'])\n",
        "        self.best_loss = checkpoint['best_loss']\n",
        "        self.start_epoch = checkpoint['epoch']\n",
        "        self.total_time = checkpoint['total_time']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_wXa0bgxzDtI"
      },
      "outputs": [],
      "source": [
        "normalize_color = torchvision.transforms.Normalize((-1,), (2,)) # from <-1, 1> to <0, 1>\n",
        "\n",
        "def psnr_bpp(model, dataloader):\n",
        "    model.update()\n",
        "    compressed_size = 0\n",
        "    image_size = 0\n",
        "    total_psnr = 0\n",
        "    batch_count = 0\n",
        "    for batch, _ in dataloader.val_loader:\n",
        "        images = batch.to(device)\n",
        "        image_size += images.shape[0] * images.shape[2] * images.shape[3]\n",
        "        output = model.compress(images)\n",
        "        for strings in output[\"strings\"]:\n",
        "            for string in strings:\n",
        "                compressed_size += len(string) * 8\n",
        "\n",
        "        images_rec = model.decompress(output[\"strings\"], output[\"shape\"])[\"x_hat\"]\n",
        "        total_psnr += piq.psnr(normalize_color(images), normalize_color(images_rec)).item()\n",
        "        batch_count += 1\n",
        "\n",
        "    bpp = compressed_size / image_size\n",
        "    psnr = total_psnr / batch_count\n",
        "    return psnr, bpp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW0QzTSH51Zn"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xCtEtXjc59lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a09d3f-0189-495f-baaa-2185091a581e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:02<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ],
      "source": [
        "model = compressai.zoo.bmshj2018_factorized(quality=3).to(device)\n",
        "model_reduced = FactorizedPriorReduced(64, 96).to(device)\n",
        "\n",
        "opt_conf = {\n",
        "    \"net\": {\"type\": \"Adam\", \"lr\": 0.0001},\n",
        "    \"aux\": {\"type\": \"Adam\", \"lr\": 0.001},\n",
        "}\n",
        "optimizer = net_aux_optimizer(model_reduced, opt_conf)\n",
        "main_optimizer = optimizer[\"net\"]\n",
        "aux_optimizer = optimizer[\"aux\"]\n",
        "\n",
        "train_conf = TrainConfig(\n",
        "    model = model_reduced,\n",
        "    epochs = 75,\n",
        "    dataloader = stl10,\n",
        "    loss_metric = lpips.LPIPS(net=\"alex\").to(device),\n",
        "    loss_alpha = 10,\n",
        "    main_optimizer = main_optimizer,\n",
        "    aux_optimizer = aux_optimizer,\n",
        "    main_scheduler = optim.lr_scheduler.MultiplicativeLR(main_optimizer, lambda epoch: 0.98),\n",
        "    aux_scheduler = optim.lr_scheduler.MultiplicativeLR(aux_optimizer, lambda epoch: 0.98),\n",
        "    save_path = \"/content/drive/My Drive/knn_models\",\n",
        "    custom_name = \"reduced_64_96_10_lpips_2\",\n",
        "    load_model = \"/content/drive/My Drive/knn_models/reduced_64_96_10_lpips_last.pth\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PtvT_F3wUJd"
      },
      "source": [
        "Loss function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edGy0WlgrwdV"
      },
      "source": [
        "Loading checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lafmw4ZurwJ6"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path : str, model, main_opt, aux_opt, main_sch, aux_sch, best_loss, epoch, total_time):\n",
        "    torch.save({\n",
        "        'model_state': model.state_dict(),\n",
        "        'main_optimizer_state': main_opt.state_dict(),\n",
        "        'aux_optimizer_state': aux_opt.state_dict(),\n",
        "        'main_scheduler_state': main_sch.state_dict() if main_sch else None,\n",
        "        'aux_scheduler_state': aux_sch.state_dict() if aux_sch else None,\n",
        "        'best_loss': best_loss,\n",
        "        'epoch': epoch,\n",
        "        'total_time': total_time,\n",
        "        },\n",
        "    path)\n",
        "\n",
        "\n",
        "def load_checkpoint(path : str, model, main_opt, aux_opt, main_sch, aux_sch):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "    main_opt.load_state_dict(checkpoint['main_optimizer_state'])\n",
        "    aux_opt.load_state_dict(checkpoint['aux_optimizer_state'])\n",
        "    if main_sch and checkpoint['main_scheduler_state']:\n",
        "        main_sch.load_state_dict(checkpoint['main_scheduler_state'])\n",
        "    if aux_sch and checkpoint['aux_scheduler_state']:\n",
        "        aux_sch.load_state_dict(checkpoint['aux_scheduler_state'])\n",
        "    best_loss = checkpoint['best_loss']\n",
        "    epoch = checkpoint['epoch']\n",
        "    total_time = checkpoint['total_time']\n",
        "    return best_loss, epoch, total_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FCS1ZVDQEw-P"
      },
      "outputs": [],
      "source": [
        "def log_header(file, conf):\n",
        "    file.write(f\"========= Model =========\\n\")\n",
        "    file.write(f\"Loaded from checkpoint: {conf.load_model if conf.load_model else 'No'}\\n\")\n",
        "    file.write(str(conf.model))\n",
        "    file.write(f\"========= Data Loader =========\\n\")\n",
        "    file.write(f\"Name: {conf.dataloader.name}\\n\")\n",
        "    file.write(f\"Batch size: {conf.dataloader.batch_size}\\n\")\n",
        "    file.write(f\"Training samples: {len(conf.dataloader.train_loader.dataset)}\\n\")\n",
        "    file.write(f\"Validation samples: {len(conf.dataloader.val_loader.dataset)}\\n\")\n",
        "    file.write(f\"Test samples: {len(conf.dataloader.test_loader.dataset)}\\n\")\n",
        "    file.write(f\"========= Loss Function =========\\n\")\n",
        "    file.write(f\"Metric: {conf.loss_metric._get_name() if hasattr(conf.loss_metric, '_get_name') else conf.loss_metric.__class__.__name__}\\n\")\n",
        "    file.write(f\"Alpha: {conf.loss_alpha}\\n\")\n",
        "    file.write(f\"Aux target: {conf.model.entropy_bottleneck.target.tolist()}\\n\")\n",
        "    file.write(f\"========= Optimizers =========\\n\")\n",
        "    file.write(f\"Main optimizer: {conf.main_optimizer.__class__.__name__}\\n\")\n",
        "    file.write(f\"Main learning rate: {conf.main_optimizer.param_groups[0]['lr']}\\n\")\n",
        "    file.write(f\"Aux optimizer: {conf.aux_optimizer.__class__.__name__}\\n\")\n",
        "    file.write(f\"Aux learning rate: {conf.aux_optimizer.param_groups[0]['lr']}\\n\")\n",
        "    file.write(f\"========= Schedulers =========\\n\")\n",
        "    file.write(f\"Main scheduler: {conf.main_scheduler.__class__.__name__ if conf.main_scheduler else 'None'}\\n\")\n",
        "    file.write(f\"Main parameters: {conf.main_scheduler.state_dict() if conf.main_scheduler else 'None'}\\n\")\n",
        "    file.write(f\"Aux scheduler: {conf.main_scheduler.__class__.__name__ if conf.main_scheduler else 'None'}\\n\")\n",
        "    file.write(f\"Aux parameters: {conf.main_scheduler.state_dict() if conf.main_scheduler else 'None'}\\n\")\n",
        "    file.write(f\"========= Training =========\\n\")\n",
        "\n",
        "def log_epoch(file, index, total, main_loss, dist_loss, bpp_loss, aux_loss, main_lr, aux_lr, time):\n",
        "    file.write(f\"Epoch [{index + 1}/{total}]: main_loss {main_loss:.6f}, distortion {dist_loss:.6f}, bpp {bpp_loss:.6f}, aux_loss {aux_loss:.6f}, main lr {main_lr:.4g}, aux lr {aux_lr:.4g} time {time:.2f}\\n\")\n",
        "    file.flush()\n",
        "\n",
        "def log_finalize(file, best_loss, psnr, bpp, time):\n",
        "    file.write(f\"========= Summary =========\\n\")\n",
        "    file.write(f\"Best loss: {best_loss:.6f}\\n\")\n",
        "    file.write(f\"Final psnr: {psnr:.2f}\\n\")\n",
        "    file.write(f\"Final bpp: {bpp:.4f}\\n\")\n",
        "    file.write(f\"Total time: {time:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Tlz4BUasCbcu"
      },
      "outputs": [],
      "source": [
        "def loss_function(x, x_recon, likelihoods, metric, alpha, transform = None):\n",
        "    pixel_count = x.shape[0] * x.shape[2] * x.shape[3]\n",
        "    if transform:\n",
        "        distortion = metric(transform(x_recon), transform(x)).mean() * alpha\n",
        "    else:\n",
        "        distortion = metric(x_recon, x).mean() * alpha\n",
        "    bpp = sum((-torch.log(likelihood).sum() / (pixel_count * math.log(2))) for likelihood in likelihoods.values())\n",
        "    return distortion + bpp, distortion, bpp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVrLuDyF6BlP"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LYC-93e26DfO"
      },
      "outputs": [],
      "source": [
        "def train(conf : TrainConfig):\n",
        "    log_file = open(f\"{conf.save_path}/logs/{conf.custom_name}.log\", \"w\")\n",
        "    log_header(log_file, conf)\n",
        "\n",
        "    model, dataloader, loss_metric, loss_alpha, loss_transform = conf.model, conf.dataloader, conf.loss_metric, conf.loss_alpha, conf.loss_transform\n",
        "    main_optimizer, aux_optimizer, main_scheduler, aux_scheduler = conf.main_optimizer, conf.aux_optimizer, conf.main_scheduler, conf.aux_scheduler\n",
        "    start_epoch, end_epoch = conf.start_epoch, conf.start_epoch + conf.epochs\n",
        "    best_loss, total_time = conf.best_loss, conf.total_time\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        for i, (x, _) in enumerate(dataloader.train_loader):\n",
        "            print(f\"\\rProcessing batch {i + 1}/{len(dataloader.train_loader)}\", end=\"\")\n",
        "            x = x.to(device)\n",
        "\n",
        "            output = model(x)\n",
        "            x_recon = output[\"x_hat\"]\n",
        "            likelihoods = output[\"likelihoods\"]\n",
        "\n",
        "            main_loss, _, _ = loss_function(x, x_recon, likelihoods, loss_metric, loss_alpha, loss_transform)\n",
        "\n",
        "            main_optimizer.zero_grad()\n",
        "            main_loss.backward()\n",
        "            main_optimizer.step()\n",
        "\n",
        "            aux_loss = model.aux_loss()\n",
        "\n",
        "            aux_optimizer.zero_grad()\n",
        "            aux_loss.backward()\n",
        "            aux_optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, _ in dataloader.val_loader:\n",
        "                x = x.to(device)\n",
        "\n",
        "                output = model(x)\n",
        "                x_recon = output[\"x_hat\"]\n",
        "                likelihoods = output[\"likelihoods\"]\n",
        "\n",
        "                main_loss, dist_loss, bpp_loss = loss_function(x, x_recon, likelihoods, loss_metric, loss_alpha, loss_transform)\n",
        "                aux_loss = model.aux_loss()\n",
        "\n",
        "            end_time = time.time()\n",
        "            epoch_duration = end_time - start_time\n",
        "            total_time += epoch_duration\n",
        "            print(f\"\\rEpoch [{epoch + 1}/{end_epoch}]: main_loss {main_loss:.6f}, distortion {dist_loss:.6f}, bpp {bpp_loss:.6f}, aux_loss {aux_loss:.6f}, time {epoch_duration:.2f}\")\n",
        "            log_epoch(log_file, epoch, end_epoch, main_loss, dist_loss, bpp_loss, aux_loss, main_optimizer.param_groups[0]['lr'], aux_optimizer.param_groups[0]['lr'], epoch_duration)\n",
        "\n",
        "        if main_loss < best_loss:\n",
        "            best_loss = main_loss\n",
        "            save_checkpoint(f\"{conf.save_path}/{conf.custom_name}_best.pth\", model, main_optimizer, aux_optimizer, main_scheduler, aux_scheduler, best_loss, epoch, total_time)\n",
        "\n",
        "        if main_scheduler:\n",
        "            main_scheduler.step()\n",
        "\n",
        "        if aux_scheduler:\n",
        "            aux_scheduler.step()\n",
        "\n",
        "    psnr, bpp = psnr_bpp(model, dataloader)\n",
        "\n",
        "    log_finalize(log_file, best_loss, psnr, bpp, total_time)\n",
        "\n",
        "    save_checkpoint(f\"{conf.save_path}/{conf.custom_name}_last.pth\", model, main_optimizer, aux_optimizer, main_scheduler, aux_scheduler, best_loss, end_epoch, total_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j12fGPE6Gga"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCMcIq6lHheY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e39bb0-e406-45c0-8152-e0cfef6d532f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [76/150]: main_loss 0.852031, distortion 0.491894, bpp 0.360137, aux_loss 2.788088, time 156.75\n",
            "Epoch [77/150]: main_loss 0.854466, distortion 0.494381, bpp 0.360085, aux_loss 1.980722, time 151.09\n",
            "Epoch [78/150]: main_loss 0.854120, distortion 0.493851, bpp 0.360269, aux_loss 2.535418, time 150.80\n",
            "Epoch [79/150]: main_loss 0.851250, distortion 0.491060, bpp 0.360190, aux_loss 2.296836, time 150.87\n",
            "Epoch [80/150]: main_loss 0.852865, distortion 0.492721, bpp 0.360144, aux_loss 2.149339, time 149.13\n",
            "Epoch [81/150]: main_loss 0.851733, distortion 0.491451, bpp 0.360281, aux_loss 1.983813, time 149.08\n",
            "Epoch [82/150]: main_loss 0.846693, distortion 0.486750, bpp 0.359944, aux_loss 2.099408, time 148.73\n",
            "Epoch [83/150]: main_loss 0.854527, distortion 0.493775, bpp 0.360752, aux_loss 2.326850, time 150.84\n",
            "Epoch [84/150]: main_loss 0.849842, distortion 0.489127, bpp 0.360715, aux_loss 2.137887, time 150.12\n",
            "Epoch [85/150]: main_loss 0.856490, distortion 0.496627, bpp 0.359863, aux_loss 2.017528, time 149.15\n",
            "Epoch [86/150]: main_loss 0.849121, distortion 0.489059, bpp 0.360063, aux_loss 2.080579, time 152.96\n",
            "Epoch [87/150]: main_loss 0.842830, distortion 0.483165, bpp 0.359665, aux_loss 1.922226, time 149.82\n",
            "Epoch [88/150]: main_loss 0.848939, distortion 0.488744, bpp 0.360195, aux_loss 2.063381, time 149.54\n",
            "Epoch [89/150]: main_loss 0.843933, distortion 0.483944, bpp 0.359989, aux_loss 2.007998, time 149.67\n",
            "Epoch [90/150]: main_loss 0.847719, distortion 0.487641, bpp 0.360078, aux_loss 2.246596, time 148.30\n",
            "Epoch [91/150]: main_loss 0.845121, distortion 0.485082, bpp 0.360039, aux_loss 2.062616, time 149.97\n",
            "Epoch [92/150]: main_loss 0.852992, distortion 0.493894, bpp 0.359097, aux_loss 1.872045, time 150.29\n",
            "Epoch [93/150]: main_loss 0.851462, distortion 0.492002, bpp 0.359460, aux_loss 1.733566, time 149.67\n",
            "Epoch [94/150]: main_loss 0.849315, distortion 0.489729, bpp 0.359586, aux_loss 1.985609, time 148.98\n",
            "Epoch [95/150]: main_loss 0.845539, distortion 0.486489, bpp 0.359050, aux_loss 2.272768, time 149.74\n",
            "Epoch [96/150]: main_loss 0.844307, distortion 0.484782, bpp 0.359525, aux_loss 1.988615, time 149.26\n",
            "Epoch [97/150]: main_loss 0.837884, distortion 0.479373, bpp 0.358511, aux_loss 2.067727, time 148.82\n",
            "Epoch [98/150]: main_loss 0.848253, distortion 0.488374, bpp 0.359879, aux_loss 2.039346, time 148.49\n",
            "Epoch [99/150]: main_loss 0.848360, distortion 0.489204, bpp 0.359156, aux_loss 2.003738, time 149.06\n",
            "Epoch [100/150]: main_loss 0.842408, distortion 0.483227, bpp 0.359181, aux_loss 2.021971, time 146.77\n",
            "Epoch [101/150]: main_loss 0.842774, distortion 0.483696, bpp 0.359078, aux_loss 1.775230, time 149.48\n",
            "Epoch [102/150]: main_loss 0.844462, distortion 0.485044, bpp 0.359417, aux_loss 1.810357, time 149.21\n",
            "Epoch [103/150]: main_loss 0.849622, distortion 0.490409, bpp 0.359213, aux_loss 1.885247, time 147.92\n",
            "Epoch [104/150]: main_loss 0.841887, distortion 0.482950, bpp 0.358938, aux_loss 1.855824, time 148.16\n",
            "Epoch [105/150]: main_loss 0.838682, distortion 0.480057, bpp 0.358625, aux_loss 1.685446, time 146.90\n",
            "Epoch [106/150]: main_loss 0.846211, distortion 0.487661, bpp 0.358550, aux_loss 1.863963, time 147.90\n",
            "Epoch [107/150]: main_loss 0.842539, distortion 0.483710, bpp 0.358829, aux_loss 2.343526, time 147.68\n",
            "Epoch [108/150]: main_loss 0.842201, distortion 0.483677, bpp 0.358524, aux_loss 1.763230, time 147.88\n",
            "Epoch [109/150]: main_loss 0.842891, distortion 0.484310, bpp 0.358581, aux_loss 1.916031, time 146.81\n",
            "Epoch [110/150]: main_loss 0.839813, distortion 0.480869, bpp 0.358943, aux_loss 1.786856, time 147.58\n",
            "Processing batch 564/1250"
          ]
        }
      ],
      "source": [
        "\n",
        "train(train_conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK2JSSJg2k-8"
      },
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBzEvpii2kbk"
      },
      "outputs": [],
      "source": [
        "#train_conf.load_from_checkpoint(\"/content/drive/My Drive/knn_models/reduced_64_96_100_mse_best.pth\")\n",
        "for batch, _ in train_conf.dataloader.val_loader:\n",
        "    images = batch.to(device)\n",
        "    output = train_conf.model(images)[\"x_hat\"]\n",
        "    images = torch.cat((normalize_color(images), normalize_color(output)), dim=2)\n",
        "\n",
        "    npimage = torchvision.utils.make_grid(images).cpu().numpy()\n",
        "    npimage = npimage.transpose(1, 2, 0)\n",
        "    plt.figure(figsize=(20, 16))\n",
        "    plt.imshow(npimage)\n",
        "    plt.show()\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryVM85pq8BNI"
      },
      "outputs": [],
      "source": [
        "baseline_metrics = {    ## 500 images from the validation set of stl10 dataset (unlabeled split)\n",
        "\"bms\": {\n",
        "    \"bpp\": [0.19988194444444443, 0.2988402777777776, 0.44673611111111056, 0.6683263888888891, 0.9627986111111108, 1.4655763888888897, 2.0330416666666653, 2.7216875000000007],\n",
        "    \"psnr\": [23.897150814056396, 25.15504557800293, 26.526659744262695, 28.087163276672364, 29.60244454574585, 31.903252914428712, 33.75015351486206, 35.73424911880493]},\n",
        "\"jpeg\": {\n",
        "    \"bpp\": [1.0225347222222223, 1.300105902777778, 1.5240902777777778, 1.7105746527777783, 1.886397569444444, 2.076463541666668, 2.363946180555556, 2.8256059027777805, 3.8610625000000014],\n",
        "    \"psnr\": [24.0933437461853, 26.12165872192383, 27.2635897064209, 28.046062282562257, 28.69297319030762, 29.346161067962647, 30.227422931671143, 31.532780990600585, 33.8721844291687]}\n",
        "}\n",
        "\n",
        "psnr, bpp = psnr_bpp(train_conf.model, train_conf.dataloader)\n",
        "baseline_metrics[\"our\"] = {\"bpp\": [bpp], \"psnr\": [psnr]}\n",
        "\n",
        "def plot_psnr_bpp(model_metrics):\n",
        "    plt.xlabel('BPP (bits per pixel)')\n",
        "    plt.ylabel('PSNR')\n",
        "    plt.title('BPP vs PSNR')\n",
        "    for model, metrics in model_metrics.items():\n",
        "        plt.plot(metrics[\"bpp\"], metrics[\"psnr\"], 'o-', markersize=4, label=model)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_psnr_bpp(baseline_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(baseline_metrics[\"our\"])"
      ],
      "metadata": {
        "id": "Lp9EEUrzFxLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}